{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece314a9",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb767bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is: 2.9.1\n",
      "Eager execution is: True\n",
      "Keras version is: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow import slice\n",
    "\n",
    "print(\"TensorFlow version is: {}\".format(tf.__version__))\n",
    "print(\"Eager execution is: {}\".format(tf.executing_eagerly()))\n",
    "print(\"Keras version is: {}\".format(tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf5203",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71edebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 16:33:25.199856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#yolo_tiny_model = load_model('checkpoints/ppe_yolov4-tiny-608-24thAug')\n",
    "yolo_tiny_model = load_model('checkpoints/yolov4_complete.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2455137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 448, 448, 3)]     0         \n",
      "                                                                 \n",
      " model (Functional)          (None, None, 16)          70745849  \n",
      "                                                                 \n",
      " yolo_decoder (Functional)   [(None, None),            0         \n",
      "                              (None, 12)]                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,745,849\n",
      "Trainable params: 70,670,841\n",
      "Non-trainable params: 75,008\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "yolo_tiny_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5bbaf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 448, 448, 3) dtype=float32 (created by layer 'input_1')>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_tiny_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26564464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, None) dtype=float32 (created by layer 'yolo_decoder')>,\n",
       " <KerasTensor: shape=(None, 12) dtype=float32 (created by layer 'yolo_decoder')>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_tiny_model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d99a56",
   "metadata": {},
   "source": [
    "# read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce781052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(labels_path):\n",
    "    with open(labels_path) as f:\n",
    "        labels = f.readlines()\n",
    "    labels = [c.strip() for c in labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d7366b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barrier',\n",
       " 'barrier_angle',\n",
       " 'barrier_connector',\n",
       " 'barrier_pad',\n",
       " 'cone',\n",
       " 'excavator',\n",
       " 'excavator_nwl',\n",
       " 'fire_extinguisher',\n",
       " 'forklift',\n",
       " 'ladder',\n",
       " 'manhole',\n",
       " 'water_barrier']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels = read_labels('data/classes/ppe_classes.names.txt')\n",
    "labels = read_labels('data/classes/site_od_classes.txt')\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456c0b9",
   "metadata": {},
   "source": [
    "# decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b00ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(labels)\n",
    "total_len = 4 + num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ead3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tf.keras.layers.Input(shape=(None, total_len), name='model_input')\n",
    "slice_x0 = tf.keras.layers.Lambda( lambda x: x[..., 0], name = 'all_x0')(inputs)\n",
    "slice_y0 = tf.keras.layers.Lambda(lambda x: x[..., 1], name = 'all_y0')(inputs)\n",
    "\n",
    "\n",
    "slice_width = tf.keras.layers.Lambda(lambda x: x[..., 2], name = 'all_widths')(inputs)\n",
    "\n",
    "\n",
    "slice_height = tf.keras.layers.Lambda( lambda x: x[..., 3], name = 'all_heights')(inputs)\n",
    "\n",
    "# slice_height = tf.keras.layers.Subtract()([slice_y1, slice_y0])\n",
    "# slice_width = tf.keras.layers.Subtract()([slice_x1, slice_x0])\n",
    "\n",
    "# concatenate_x0_y0 = tf.keras.layers.Concatenate(axis = 0, name='concatenate_x0_y0')([slice_x0, slice_y0, slice_x1, slice_y1])\n",
    "# transpose_dims = tf.keras.layers.Lambda(lambda x: tf.transpose(x), name ='all_boxes')(concatenate_x0_y0)\n",
    "\n",
    "concatenate_x0_y0_x1_y1 = tf.keras.layers.Concatenate(axis = 0, name='concatenate_x0_y0_x1_y1')([slice_x0, slice_y0, slice_width, slice_height])\n",
    "x_y_w_h = tf.keras.layers.Lambda(lambda x: tf.transpose(x), name ='x_y_w_h')(concatenate_x0_y0_x1_y1)\n",
    "# normalized_x_y_w_h = tf.keras.layers.Lambda(lambda x: x/608.0, name ='normalized_x_y_w_h')(x_y_w_h)\n",
    "\n",
    "slice_confidences = tf.keras.layers.Lambda(lambda x: x[..., 4:], name = 'slice_confidences')(inputs)\n",
    "scores = tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=0), name = 'all_scores')(slice_confidences)\n",
    "\n",
    "\n",
    "decoder_model = tf.keras.Model(inputs = [inputs], outputs = [x_y_w_h, scores ], name = 'yolo_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05f9402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"yolo_decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " model_input (InputLayer)       [(None, None, 16)]   0           []                               \n",
      "                                                                                                  \n",
      " all_x0 (Lambda)                (None, None)         0           ['model_input[0][0]']            \n",
      "                                                                                                  \n",
      " all_y0 (Lambda)                (None, None)         0           ['model_input[0][0]']            \n",
      "                                                                                                  \n",
      " all_widths (Lambda)            (None, None)         0           ['model_input[0][0]']            \n",
      "                                                                                                  \n",
      " all_heights (Lambda)           (None, None)         0           ['model_input[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_x0_y0_x1_y1 (Conca  (None, None)        0           ['all_x0[0][0]',                 \n",
      " tenate)                                                          'all_y0[0][0]',                 \n",
      "                                                                  'all_widths[0][0]',             \n",
      "                                                                  'all_heights[0][0]']            \n",
      "                                                                                                  \n",
      " slice_confidences (Lambda)     (None, None, 12)     0           ['model_input[0][0]']            \n",
      "                                                                                                  \n",
      " x_y_w_h (Lambda)               (None, None)         0           ['concatenate_x0_y0_x1_y1[0][0]']\n",
      "                                                                                                  \n",
      " all_scores (Lambda)            (None, 12)           0           ['slice_confidences[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed1561",
   "metadata": {},
   "source": [
    "# merege yolo and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39d69ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 16) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 16), dtype=tf.float32, name='model_input'), name='model_input', description=\"created by layer 'model_input'\"), but it was called on an input with incompatible shape (None, None).\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 448, 448, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model (Functional)             [(None, None),       70745849    ['input_1[0][0]']                \n",
      "                                 (None, 12)]                                                      \n",
      "                                                                                                  \n",
      " yolo_decoder (Functional)      [(None, None),       0           ['model[0][0]',                  \n",
      "                                 (None, 12)]                      'model[0][1]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,745,849\n",
      "Trainable params: 70,670,841\n",
      "Non-trainable params: 75,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = yolo_tiny_model.outputs[0]\n",
    "x.shape\n",
    "decoder_model.input.shape\n",
    "\n",
    "#inputs = tf.keras.layers.Input(shape=(608, 608, 3))\n",
    "inputs = yolo_tiny_model.input\n",
    "x = yolo_tiny_model(inputs)\n",
    "predictions = decoder_model(x)\n",
    "combined_model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fbafdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "#save combined model\n",
    "combined_model.save('yolo_decoded.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc197e32",
   "metadata": {},
   "source": [
    "# check predictions¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f95140b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import random\n",
    "\n",
    "def generate_colors(class_names):\n",
    "    hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] ), int(x[1] ), int(x[2] )), colors))\n",
    "    random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "    random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "    random.seed(None)  # Reset seed to default.\n",
    "    return colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6c3a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, interpolation = 'bilinear', target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    \n",
    "    return image, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef44649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def draw_preds_bbs(file_name, preds_pipeline, is_coreml = False):\n",
    "    print(preds_pipeline[0].shape)\n",
    "    print(preds_pipeline[1].shape)\n",
    "    boxes = preds_pipeline[0]\n",
    "    scores = preds_pipeline[1]\n",
    "#     boxes = preds_pipeline[2]\n",
    "\n",
    "    print('start drawing boxes')\n",
    "    data = pyplot.imread(file_name)\n",
    "    ax = pyplot.gca()\n",
    "    colors = generate_colors(labels)\n",
    "    for i in range(len(boxes)):\n",
    "        index = i\n",
    "        box = boxes[index]\n",
    "        class_id = np.argmax(scores[i])\n",
    "        score = scores[i][class_id]\n",
    "        if is_coreml == False :\n",
    "            x1, y1, height, width = box[0], box[1], box[2], box[3]\n",
    "        else:\n",
    "            x1, y1, width, height = box[0], box[1], box[2], box[3]\n",
    "        \n",
    "        x1, y1, width, height = box[0], box[1], box[2], box[3]\n",
    "        \n",
    "        x1 = x1- width/2.0\n",
    "        y1 = y1- height/2.0\n",
    "        \n",
    "        rect = Rectangle(\n",
    "            (x1 * image_w, y1 * image_h),\n",
    "            width * image_w,\n",
    "            height * image_h,\n",
    "            fill=False,\n",
    "            color=colors[class_id]\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        label = \"%s (%.3f)\" % (labels[class_id], score*100)\n",
    "        pyplot.text(\n",
    "            x1* image_w,\n",
    "            y1* image_h,\n",
    "            label, \n",
    "            color= 'white'\n",
    "        )\n",
    "    pyplot.imshow(data)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59e0c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image initial size:  1280 720\n",
      "input image (1, 448, 448, 3)\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the image\n",
    "input_w, input_h = combined_model.input.shape[1], combined_model.input.shape[2]#608, 608\n",
    "photo_filename = 'sample_test_images/0.jpg'\n",
    "# photo_filename = 'empty_image.jpg'\n",
    "image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n",
    "print(\"image initial size: \", image_w, image_h)\n",
    "print(\"input image\",image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f222aa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Exception encountered when calling layer \"model\" (type Functional).\n    \n    Layer \"yolo_decoder\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'model/model/yolo_decoder/x_y_w_h/transpose:0' shape=(None, None) dtype=float32>, <tf.Tensor 'model/model/yolo_decoder/all_scores/Squeeze:0' shape=(None, 12) dtype=float32>]\n    \n    Call arguments received by layer \"model\" (type Functional):\n      • inputs=tf.Tensor(shape=(None, 448, 448, 3), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ytinyhat \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(f\"ytinyhat shape : {ytinyhat.shape}\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m draw_preds_bbs(photo_filename, ytinyhat)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/zz/4lg5ylr92_d7sjz9ds3lb3h00000gn/T/__autograph_generated_fileexirlvay.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/ankit/opt/anaconda3/envs/coremlConverterPipelineFinal/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Exception encountered when calling layer \"model\" (type Functional).\n    \n    Layer \"yolo_decoder\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'model/model/yolo_decoder/x_y_w_h/transpose:0' shape=(None, None) dtype=float32>, <tf.Tensor 'model/model/yolo_decoder/all_scores/Squeeze:0' shape=(None, 12) dtype=float32>]\n    \n    Call arguments received by layer \"model\" (type Functional):\n      • inputs=tf.Tensor(shape=(None, 448, 448, 3), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "ytinyhat = combined_model.predict(image)\n",
    "# print(f\"ytinyhat shape : {ytinyhat.shape}\")\n",
    "draw_preds_bbs(photo_filename, ytinyhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8b1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
